{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on **Tuesday, April 12th, 2022, 6:29pm PST**. You may work with one other person.\n",
    "## TF-IDF (5pts)\n",
    "\n",
    "You are an analyst working for Amazon's product team, and charged with identifying areas for improvement for the toy reviews.\n",
    "\n",
    "Using the **amazon-fine-foods.csv** dataset, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "- which words to collocate together?\n",
    "\n",
    "Finally, generate a TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from textacy import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/amazon_fine_foods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A21U4DR8M6I9QN</td>\n",
       "      <td>K. M Merrill \"justine\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318896000</td>\n",
       "      <td>addictive! but works for night coughing in dogs</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A17TDUBB4Z1PEC</td>\n",
       "      <td>jaded_green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318550400</td>\n",
       "      <td>genuine Greenies best price</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20985</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>ABQH3WAWMSMBH</td>\n",
       "      <td>tenisbrat87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1317168000</td>\n",
       "      <td>Perfect for our little doggies</td>\n",
       "      <td>Our dogs love Greenies, but of course, which d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20986</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>AVTY5M74VA1BJ</td>\n",
       "      <td>tarotqueen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1316822400</td>\n",
       "      <td>dogs love greenies</td>\n",
       "      <td>What can I say, dogs love greenies. They begg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20987</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A13TNN54ZEAUB1</td>\n",
       "      <td>dcz2221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1316736000</td>\n",
       "      <td>Greenies review</td>\n",
       "      <td>This review is for a box of Greenies Lite for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId             ProfileName  \\\n",
       "0  20983  B002QWP89S  A21U4DR8M6I9QN  K. M Merrill \"justine\"   \n",
       "1  20984  B002QWP89S  A17TDUBB4Z1PEC             jaded_green   \n",
       "2  20985  B002QWP89S   ABQH3WAWMSMBH             tenisbrat87   \n",
       "3  20986  B002QWP89S   AVTY5M74VA1BJ              tarotqueen   \n",
       "4  20987  B002QWP89S  A13TNN54ZEAUB1                 dcz2221   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1318896000   \n",
       "1                     1                       1      5  1318550400   \n",
       "2                     1                       1      5  1317168000   \n",
       "3                     1                       1      5  1316822400   \n",
       "4                     1                       1      5  1316736000   \n",
       "\n",
       "                                           Summary  \\\n",
       "0  addictive! but works for night coughing in dogs   \n",
       "1                      genuine Greenies best price   \n",
       "2                   Perfect for our little doggies   \n",
       "3                               dogs love greenies   \n",
       "4                                  Greenies review   \n",
       "\n",
       "                                                Text  \n",
       "0  my 12 year old sheltie has chronic brochotitis...  \n",
       "1  These are genuine Greenies product, not a knoc...  \n",
       "2  Our dogs love Greenies, but of course, which d...  \n",
       "3  What can I say, dogs love greenies. They begg ...  \n",
       "4  This review is for a box of Greenies Lite for ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chips, sweets, coconut oil\n",
    "df = df[(df['ProductId'] == 'B006HYLW32') | (df['ProductId'] == 'B005K4Q1YA') | (df['ProductId'] == 'B001EO5Q64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using textacy to: remove hyphens, punctuation, and accents\n",
    "preproc = preprocessing.make_pipeline(\n",
    "    preprocessing.remove.html_tags,\n",
    "    preprocessing.normalize.hyphenated_words,\n",
    "    preprocessing.remove.punctuation,\n",
    "    preprocessing.remove.accents,\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))\n",
    "# removing some negative words from stopwords list\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_stopwords.remove('below')\n",
    "nltk_stopwords.remove(\"aren't\")\n",
    "nltk_stopwords.remove('couldn')\n",
    "nltk_stopwords.remove(\"couldn't\")\n",
    "nltk_stopwords.remove(\"didn't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it may be easier to use the pre-built function, it does not allow for changing the stopwords list. Therefore I am creating the function manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords \n",
    "def remove_stopwords(sentence:str, nltk_stopwords):\n",
    "    '''removing stopwords from a list, review: string, nltk_stopwords: list'''\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in nltk_stopwords:\n",
    "            continue\n",
    "        new_words.append(word)\n",
    "    cleaned_review = \" \".join(new_words)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A21U4DR8M6I9QN</td>\n",
       "      <td>K. M Merrill \"justine\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318896000</td>\n",
       "      <td>addictive! but works for night coughing in dogs</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "      <td>12 year old sheltie chronic brochotitis meds t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A17TDUBB4Z1PEC</td>\n",
       "      <td>jaded_green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318550400</td>\n",
       "      <td>genuine Greenies best price</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "      <td>genuine Greenies product knockoff dogs love fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20985</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>ABQH3WAWMSMBH</td>\n",
       "      <td>tenisbrat87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1317168000</td>\n",
       "      <td>Perfect for our little doggies</td>\n",
       "      <td>Our dogs love Greenies, but of course, which d...</td>\n",
       "      <td>dogs love Greenies course doggies bought dashc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20986</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>AVTY5M74VA1BJ</td>\n",
       "      <td>tarotqueen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1316822400</td>\n",
       "      <td>dogs love greenies</td>\n",
       "      <td>What can I say, dogs love greenies. They begg ...</td>\n",
       "      <td>say dogs love greenies begg time always sit cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20987</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A13TNN54ZEAUB1</td>\n",
       "      <td>dcz2221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1316736000</td>\n",
       "      <td>Greenies review</td>\n",
       "      <td>This review is for a box of Greenies Lite for ...</td>\n",
       "      <td>review box Greenies Lite dog package came quic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId             ProfileName  \\\n",
       "0  20983  B002QWP89S  A21U4DR8M6I9QN  K. M Merrill \"justine\"   \n",
       "1  20984  B002QWP89S  A17TDUBB4Z1PEC             jaded_green   \n",
       "2  20985  B002QWP89S   ABQH3WAWMSMBH             tenisbrat87   \n",
       "3  20986  B002QWP89S   AVTY5M74VA1BJ              tarotqueen   \n",
       "4  20987  B002QWP89S  A13TNN54ZEAUB1                 dcz2221   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1318896000   \n",
       "1                     1                       1      5  1318550400   \n",
       "2                     1                       1      5  1317168000   \n",
       "3                     1                       1      5  1316822400   \n",
       "4                     1                       1      5  1316736000   \n",
       "\n",
       "                                           Summary  \\\n",
       "0  addictive! but works for night coughing in dogs   \n",
       "1                      genuine Greenies best price   \n",
       "2                   Perfect for our little doggies   \n",
       "3                               dogs love greenies   \n",
       "4                                  Greenies review   \n",
       "\n",
       "                                                Text  \\\n",
       "0  my 12 year old sheltie has chronic brochotitis...   \n",
       "1  These are genuine Greenies product, not a knoc...   \n",
       "2  Our dogs love Greenies, but of course, which d...   \n",
       "3  What can I say, dogs love greenies. They begg ...   \n",
       "4  This review is for a box of Greenies Lite for ...   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  12 year old sheltie chronic brochotitis meds t...  \n",
       "1  genuine Greenies product knockoff dogs love fa...  \n",
       "2  dogs love Greenies course doggies bought dashc...  \n",
       "3  say dogs love greenies begg time always sit cu...  \n",
       "4  review box Greenies Lite dog package came quic...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_cleaned'] = df['Text'].apply(preproc).apply(remove_stopwords, nltk_stopwords=nltk_stopwords)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the corpus is food reviews, we use regex to group certain words or phrases that are related to certain tastes.\n",
    "# good tastes\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bdelicious|tasty|yum+y*\\b', '_GOOD_TASTE_', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweet\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bsugar(y|ed)?|swe{2,}t(en(ed)?)?\\b', '_SWEET_', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savory\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bsalty*|savou?ry\\b', '_SAVORY_', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all words with more than 15 digits\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\b([a-z]|[A-Z]){15,}\\b', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad tastes\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bdisgusting|ugh\\b', '_GOOD_TASTE_', case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* stemming/lemmatization (explain in your notebook why you used stemming versus lemmatization). \n",
    "\n",
    "Same as HW2, we choose lemmatization since it can better account for transformations that are not standard. Such transformations can be quite common in reviews. For example, it is very likely for reviews to contain `better` or `worse`, neither of which can be treated with stemming. Moreover, we do not have a performance limitation as the dataset is rather small and we are not doing realtime transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given the rather small dataset, we find trigrams too specific to give meaning results. Therefore, we choose 2 (bigram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Word Embeddings (2 pts)\n",
    "\n",
    "Using\n",
    "* `TfIdfVectorizer`\n",
    "\n",
    "Identify the most similar pair of reviews from the `amazon-fine-foods.csv` dataset using both Euclidean distance and cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (3pts)\n",
    "\n",
    "You are an NLP data scientist working at Fandango. You observe the following dataset in your review comments:\n",
    "\n",
    "**Intent to Buy Tickets:**\n",
    "1.\tLove this movie. Can’t wait!\n",
    "2.\tI want to see this movie so bad.\n",
    "3.\tThis movie looks amazing.\n",
    "\n",
    "**No Intent to Buy Tickets:**\n",
    "1.\tLooks bad.\n",
    "2.\tHard pass to see this bad movie.\n",
    "3.\tSo boring!\n",
    "\n",
    "You can consider the following stopwords for removal: `to`, `this`.\n",
    "\n",
    "Is the following review an `Intent to Buy` or `No Intent to Buy`? Show your work for each computation.\n",
    "> This looks so bad.\n",
    "\n",
    "You'll need to compute:\n",
    "* Prior\n",
    "* Likelihood\n",
    "* Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
