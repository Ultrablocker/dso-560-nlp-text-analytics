{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on **Tuesday, April 12th, 2022, 6:29pm PST**. You may work with one other person.\n",
    "## TF-IDF (5pts)\n",
    "\n",
    "You are an analyst working for Amazon's product team, and charged with identifying areas for improvement for the toy reviews.\n",
    "\n",
    "Using the **amazon-fine-foods.csv** dataset, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "- which words to collocate together?\n",
    "\n",
    "Finally, generate a TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from textacy import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/amazon_fine_foods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A21U4DR8M6I9QN</td>\n",
       "      <td>K. M Merrill \"justine\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318896000</td>\n",
       "      <td>addictive! but works for night coughing in dogs</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A17TDUBB4Z1PEC</td>\n",
       "      <td>jaded_green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318550400</td>\n",
       "      <td>genuine Greenies best price</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20985</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>ABQH3WAWMSMBH</td>\n",
       "      <td>tenisbrat87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1317168000</td>\n",
       "      <td>Perfect for our little doggies</td>\n",
       "      <td>Our dogs love Greenies, but of course, which d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20986</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>AVTY5M74VA1BJ</td>\n",
       "      <td>tarotqueen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1316822400</td>\n",
       "      <td>dogs love greenies</td>\n",
       "      <td>What can I say, dogs love greenies. They begg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20987</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A13TNN54ZEAUB1</td>\n",
       "      <td>dcz2221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1316736000</td>\n",
       "      <td>Greenies review</td>\n",
       "      <td>This review is for a box of Greenies Lite for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId             ProfileName  \\\n",
       "0  20983  B002QWP89S  A21U4DR8M6I9QN  K. M Merrill \"justine\"   \n",
       "1  20984  B002QWP89S  A17TDUBB4Z1PEC             jaded_green   \n",
       "2  20985  B002QWP89S   ABQH3WAWMSMBH             tenisbrat87   \n",
       "3  20986  B002QWP89S   AVTY5M74VA1BJ              tarotqueen   \n",
       "4  20987  B002QWP89S  A13TNN54ZEAUB1                 dcz2221   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1318896000   \n",
       "1                     1                       1      5  1318550400   \n",
       "2                     1                       1      5  1317168000   \n",
       "3                     1                       1      5  1316822400   \n",
       "4                     1                       1      5  1316736000   \n",
       "\n",
       "                                           Summary  \\\n",
       "0  addictive! but works for night coughing in dogs   \n",
       "1                      genuine Greenies best price   \n",
       "2                   Perfect for our little doggies   \n",
       "3                               dogs love greenies   \n",
       "4                                  Greenies review   \n",
       "\n",
       "                                                Text  \n",
       "0  my 12 year old sheltie has chronic brochotitis...  \n",
       "1  These are genuine Greenies product, not a knoc...  \n",
       "2  Our dogs love Greenies, but of course, which d...  \n",
       "3  What can I say, dogs love greenies. They begg ...  \n",
       "4  This review is for a box of Greenies Lite for ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chips, sweets, coconut oil\n",
    "df = df[(df['ProductId'] == 'B006HYLW32') | (df['ProductId'] == 'B005K4Q1YA') | (df['ProductId'] == 'B001EO5Q64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using textacy to: remove hyphens, punctuation, and accents\n",
    "preproc = preprocessing.make_pipeline(\n",
    "    preprocessing.remove.html_tags,\n",
    "    preprocessing.normalize.hyphenated_words,\n",
    "    preprocessing.remove.punctuation,\n",
    "    preprocessing.remove.accents,\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))\n",
    "# removing some negative words from stopwords list\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_stopwords.remove('below')\n",
    "nltk_stopwords.remove(\"aren't\")\n",
    "nltk_stopwords.remove('couldn')\n",
    "nltk_stopwords.remove(\"couldn't\")\n",
    "nltk_stopwords.remove(\"didn't\")\n",
    "nltk_stopwords = list(nltk_stopwords)\n",
    "# add some abstract terms\n",
    "nltk_stopwords.append('like')\n",
    "nltk_stopwords = set(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'like',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it may be easier to use the pre-built function, it does not allow for changing the stopwords list. Therefore I am creating the function manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords \n",
    "def remove_stopwords(sentence:str, nltk_stopwords):\n",
    "    '''removing stopwords from a list, review: string, nltk_stopwords: list'''\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in nltk_stopwords:\n",
    "            continue\n",
    "        new_words.append(word)\n",
    "    cleaned_review = \" \".join(new_words)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>189340</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A2H5ROZZC74XN1</td>\n",
       "      <td>Rock Bottom</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1176076800</td>\n",
       "      <td>GREAT COCONUT OIL....Try it, you'll like it!</td>\n",
       "      <td>Nutiva is the BEST COCONUT OIL! I love it and ...</td>\n",
       "      <td>Nutiva BEST COCONUT OIL love looking high qual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>189341</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A2L7UE9O293Q2W</td>\n",
       "      <td>J. Garcia</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1277769600</td>\n",
       "      <td>Love Love Love this stuff!</td>\n",
       "      <td>I bought Nutiva Organic Coconut Oil from a loc...</td>\n",
       "      <td>bought Nutiva Organic Coconut Oil local natura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950</th>\n",
       "      <td>189342</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A3KG5Q302QXSCD</td>\n",
       "      <td>M. Wang</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1256688000</td>\n",
       "      <td>Near Perfect for popcorn, experimental for oth...</td>\n",
       "      <td>Edit:  Added a second part.&lt;br /&gt;=============...</td>\n",
       "      <td>Edit Added second part =======================...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>189343</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A35JFXTIZ8X1R9</td>\n",
       "      <td>Agi</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1349222400</td>\n",
       "      <td>Delicious</td>\n",
       "      <td>At this point I've ordered this coconut oil se...</td>\n",
       "      <td>point ordered coconut oil several time feel co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>189344</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A2TVH3F0IM2UYK</td>\n",
       "      <td>kt</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1347926400</td>\n",
       "      <td>My favorite thing ever</td>\n",
       "      <td>I don't know how I lived so long without this ...</td>\n",
       "      <td>know lived long without stuff brand leaps boun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId  ProfileName  HelpfulnessNumerator  \\\n",
       "2948  189340  B001EO5Q64  A2H5ROZZC74XN1  Rock Bottom                     3   \n",
       "2949  189341  B001EO5Q64  A2L7UE9O293Q2W    J. Garcia                     5   \n",
       "2950  189342  B001EO5Q64  A3KG5Q302QXSCD      M. Wang                     7   \n",
       "2951  189343  B001EO5Q64  A35JFXTIZ8X1R9          Agi                     2   \n",
       "2952  189344  B001EO5Q64  A2TVH3F0IM2UYK           kt                     2   \n",
       "\n",
       "      HelpfulnessDenominator  Score        Time  \\\n",
       "2948                       3      5  1176076800   \n",
       "2949                       6      5  1277769600   \n",
       "2950                       9      4  1256688000   \n",
       "2951                       2      5  1349222400   \n",
       "2952                       2      5  1347926400   \n",
       "\n",
       "                                                Summary  \\\n",
       "2948       GREAT COCONUT OIL....Try it, you'll like it!   \n",
       "2949                         Love Love Love this stuff!   \n",
       "2950  Near Perfect for popcorn, experimental for oth...   \n",
       "2951                                          Delicious   \n",
       "2952                             My favorite thing ever   \n",
       "\n",
       "                                                   Text  \\\n",
       "2948  Nutiva is the BEST COCONUT OIL! I love it and ...   \n",
       "2949  I bought Nutiva Organic Coconut Oil from a loc...   \n",
       "2950  Edit:  Added a second part.<br />=============...   \n",
       "2951  At this point I've ordered this coconut oil se...   \n",
       "2952  I don't know how I lived so long without this ...   \n",
       "\n",
       "                                           text_cleaned  \n",
       "2948  Nutiva BEST COCONUT OIL love looking high qual...  \n",
       "2949  bought Nutiva Organic Coconut Oil local natura...  \n",
       "2950  Edit Added second part =======================...  \n",
       "2951  point ordered coconut oil several time feel co...  \n",
       "2952  know lived long without stuff brand leaps boun...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_cleaned'] = df['Text'].apply(preproc).apply(remove_stopwords, nltk_stopwords=nltk_stopwords)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-3dd9864c7134>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bdelicious|tasty|yum+y*\\b', '_GOOD_TASTE_', case=False)\n"
     ]
    }
   ],
   "source": [
    "# since the corpus is food reviews, we use regex to group certain words or phrases that are related to certain tastes.\n",
    "# good tastes\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bdelicious|tasty|yum+y*\\b', '_GOOD_TASTE_', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-26eb9b4724bf>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bsugar(y|ed)?|swe{2,}t(en(ed)?)?\\b', '_SWEET_', case=False)\n"
     ]
    }
   ],
   "source": [
    "# sweet\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bsugar(y|ed)?|swe{2,}t(en(ed)?)?\\b', '_SWEET_', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-f2c1a05dd933>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bsalty*|savou?ry\\b', '_SAVORY_', case=False)\n"
     ]
    }
   ],
   "source": [
    "# savory\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bsalty*|savou?ry\\b', '_SAVORY_', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-ed35d5c6daf1>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\b([a-z]|[A-Z]){15,}\\b', '', case=False)\n"
     ]
    }
   ],
   "source": [
    "# removing all words with more than 15 digits\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\b([a-z]|[A-Z]){15,}\\b', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-420e32df7c0d>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bdisgusting|ugh\\b', '_GOOD_TASTE_', case=False)\n"
     ]
    }
   ],
   "source": [
    "# bad tastes\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\bdisgusting|ugh\\b', '_GOOD_TASTE_', case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* stemming/lemmatization (explain in your notebook why you used stemming versus lemmatization). \n",
    "\n",
    "Same as HW2, we choose lemmatization since it can better account for transformations that are not standard. Such transformations can be quite common in reviews. For example, it is very likely for reviews to contain `better` or `worse`, neither of which can be treated with stemming. Moreover, we do not have a performance limitation as the dataset is rather small and we are not doing realtime transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_cleaned'] = df['text_cleaned'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given the rather small dataset, we find trigrams too specific to give meaningful results. Therefore, we choose 2 (bigram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2),\n",
    "                             token_pattern=r'\\b[a-zA-Z_]{3,}\\b',\n",
    "                             max_df=0.4, max_features=500, stop_words=nltk_stopwords)\n",
    "corpus = list(df[\"text_cleaned\"].values)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1663</th>\n",
       "      <th>1664</th>\n",
       "      <th>1665</th>\n",
       "      <th>1666</th>\n",
       "      <th>1667</th>\n",
       "      <th>1668</th>\n",
       "      <th>1669</th>\n",
       "      <th>1670</th>\n",
       "      <th>1671</th>\n",
       "      <th>1672</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>_good_taste_ also</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_good_taste_ chip</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_savory_ pepper</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_savory_ snack</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_savory_ vinegar</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would recommend</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would taste</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would try</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write review</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year old</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0     1     2     3     4     5     6     7     8     9     \\\n",
       "_good_taste_ also   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "_good_taste_ chip   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "_savory_ pepper     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "_savory_ snack      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "_savory_ vinegar    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "...                 ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "would recommend     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "would taste         0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "would try           0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "write review        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "year old            0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                   ...  1663  1664  1665      1666  1667  1668  1669  1670  \\\n",
       "_good_taste_ also  ...   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   \n",
       "_good_taste_ chip  ...   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   \n",
       "_savory_ pepper    ...   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   \n",
       "_savory_ snack     ...   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   \n",
       "_savory_ vinegar   ...   0.0   0.0   0.0  0.222908   0.0   0.0   0.0   0.0   \n",
       "...                ...   ...   ...   ...       ...   ...   ...   ...   ...   \n",
       "would recommend    ...   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   \n",
       "would taste        ...   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   \n",
       "would try          ...   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   \n",
       "write review       ...   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   \n",
       "year old           ...   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                   1671  1672  \n",
       "_good_taste_ also   0.0   0.0  \n",
       "_good_taste_ chip   0.0   0.0  \n",
       "_savory_ pepper     0.0   0.0  \n",
       "_savory_ snack      0.0   0.0  \n",
       "_savory_ vinegar    0.0   0.0  \n",
       "...                 ...   ...  \n",
       "would recommend     0.0   0.0  \n",
       "would taste         0.0   0.0  \n",
       "would try           0.0   0.0  \n",
       "write review        0.0   0.0  \n",
       "year old            0.0   0.0  \n",
       "\n",
       "[500 rows x 1673 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>...</th>\n",
       "      <th>would definitely</th>\n",
       "      <th>would give</th>\n",
       "      <th>would good</th>\n",
       "      <th>would great</th>\n",
       "      <th>would highly</th>\n",
       "      <th>would recommend</th>\n",
       "      <th>would taste</th>\n",
       "      <th>would try</th>\n",
       "      <th>write review</th>\n",
       "      <th>year old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189340</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A2H5ROZZC74XN1</td>\n",
       "      <td>Rock Bottom</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1176076800</td>\n",
       "      <td>GREAT COCONUT OIL....Try it, you'll like it!</td>\n",
       "      <td>Nutiva is the BEST COCONUT OIL! I love it and ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189341</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A2L7UE9O293Q2W</td>\n",
       "      <td>J. Garcia</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1277769600</td>\n",
       "      <td>Love Love Love this stuff!</td>\n",
       "      <td>I bought Nutiva Organic Coconut Oil from a loc...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189342</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A3KG5Q302QXSCD</td>\n",
       "      <td>M. Wang</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1256688000</td>\n",
       "      <td>Near Perfect for popcorn, experimental for oth...</td>\n",
       "      <td>Edit:  Added a second part.&lt;br /&gt;=============...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189343</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A35JFXTIZ8X1R9</td>\n",
       "      <td>Agi</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1349222400</td>\n",
       "      <td>Delicious</td>\n",
       "      <td>At this point I've ordered this coconut oil se...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189344</td>\n",
       "      <td>B001EO5Q64</td>\n",
       "      <td>A2TVH3F0IM2UYK</td>\n",
       "      <td>kt</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1347926400</td>\n",
       "      <td>My favorite thing ever</td>\n",
       "      <td>I don't know how I lived so long without this ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 511 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId  ProfileName  HelpfulnessNumerator  \\\n",
       "0  189340  B001EO5Q64  A2H5ROZZC74XN1  Rock Bottom                     3   \n",
       "1  189341  B001EO5Q64  A2L7UE9O293Q2W    J. Garcia                     5   \n",
       "2  189342  B001EO5Q64  A3KG5Q302QXSCD      M. Wang                     7   \n",
       "3  189343  B001EO5Q64  A35JFXTIZ8X1R9          Agi                     2   \n",
       "4  189344  B001EO5Q64  A2TVH3F0IM2UYK           kt                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       3      5  1176076800   \n",
       "1                       6      5  1277769600   \n",
       "2                       9      4  1256688000   \n",
       "3                       2      5  1349222400   \n",
       "4                       2      5  1347926400   \n",
       "\n",
       "                                             Summary  \\\n",
       "0       GREAT COCONUT OIL....Try it, you'll like it!   \n",
       "1                         Love Love Love this stuff!   \n",
       "2  Near Perfect for popcorn, experimental for oth...   \n",
       "3                                          Delicious   \n",
       "4                             My favorite thing ever   \n",
       "\n",
       "                                                Text  ... would definitely  \\\n",
       "0  Nutiva is the BEST COCONUT OIL! I love it and ...  ...              0.0   \n",
       "1  I bought Nutiva Organic Coconut Oil from a loc...  ...              0.0   \n",
       "2  Edit:  Added a second part.<br />=============...  ...              0.0   \n",
       "3  At this point I've ordered this coconut oil se...  ...              0.0   \n",
       "4  I don't know how I lived so long without this ...  ...              0.0   \n",
       "\n",
       "   would give  would good  would great  would highly  would recommend  \\\n",
       "0         0.0         0.0          0.0           0.0              0.0   \n",
       "1         0.0         0.0          0.0           0.0              0.0   \n",
       "2         0.0         0.0          0.0           0.0              0.0   \n",
       "3         0.0         0.0          0.0           0.0              0.0   \n",
       "4         0.0         0.0          0.0           0.0              0.0   \n",
       "\n",
       "   would taste  would try  write review  year old  \n",
       "0          0.0        0.0           0.0       0.0  \n",
       "1          0.0        0.0           0.0       0.0  \n",
       "2          0.0        0.0           0.0       0.0  \n",
       "3          0.0        0.0           0.0       0.0  \n",
       "4          0.0        0.0           0.0       0.0  \n",
       "\n",
       "[5 rows x 511 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "tf_idf_T = tf_idf.T.reset_index(drop=True)\n",
    "df = pd.concat([df, tf_idf_T], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df[df['Score'] >= 4]\n",
    "pos_bigrams = pos.iloc[:,11:].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coconut oil         105.260802\n",
      "potato chip          33.206926\n",
      "_sweet_ potato       26.313662\n",
      "french vanilla       22.132841\n",
      "great taste          22.035176\n",
      "taste good           21.746554\n",
      "gas station          21.459665\n",
      "use coconut          20.210719\n",
      "taste great          19.944107\n",
      "highly recommend     19.420590\n",
      "love product         18.467487\n",
      "_savory_ vinegar     17.852078\n",
      "great product        17.309826\n",
      "coconut flavor       17.151517\n",
      "also use             17.075502\n",
      "really good          16.793063\n",
      "grove square         16.358549\n",
      "oil use              16.049248\n",
      "pop chips            15.591747\n",
      "extra virgin         15.203516\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(pos_bigrams[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the features your analysis showed that customers cited as reasons for a poor review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = df[df['Score'] < 3]\n",
    "neg_bigrams = neg.iloc[:,11:].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instant coffee          16.029648\n",
      "gas station              9.993391\n",
      "_savory_ vinegar         6.914928\n",
      "waste money              6.304783\n",
      "grove square             6.180801\n",
      "coffee cup               5.398198\n",
      "coffee taste             4.835267\n",
      "artificial sweetener     4.807229\n",
      "potato chip              4.778847\n",
      "way _sweet_              4.419706\n",
      "station cappuccino       4.350727\n",
      "french vanilla           4.294060\n",
      "cup coffee               4.108670\n",
      "hot water                4.026322\n",
      "one cup                  3.949818\n",
      "would recommend          3.914274\n",
      "regular coffee           3.729373\n",
      "_savory_ pepper          3.689321\n",
      "coffee instant           3.558832\n",
      "cream _sweet_            3.095921\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(neg_bigrams[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coconut oil</th>\n",
       "      <td>108.163197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potato chip</th>\n",
       "      <td>39.953417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas station</th>\n",
       "      <td>32.427284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_sweet_ potato</th>\n",
       "      <td>30.509144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french vanilla</th>\n",
       "      <td>27.211078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fat fat</th>\n",
       "      <td>2.123475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil skin</th>\n",
       "      <td>2.037426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cup brewers</th>\n",
       "      <td>1.972662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brewers count</th>\n",
       "      <td>1.963031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunflower safflower</th>\n",
       "      <td>1.922352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          score\n",
       "coconut oil          108.163197\n",
       "potato chip           39.953417\n",
       "gas station           32.427284\n",
       "_sweet_ potato        30.509144\n",
       "french vanilla        27.211078\n",
       "...                         ...\n",
       "fat fat                2.123475\n",
       "oil skin               2.037426\n",
       "cup brewers            1.972662\n",
       "brewers count          1.963031\n",
       "sunflower safflower    1.922352\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_csv('scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the most common issues identified from your analysis that generated customer dissatisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Word Embeddings (2 pts)\n",
    "\n",
    "Using\n",
    "* `TfIdfVectorizer`\n",
    "\n",
    "Identify the most similar pair of reviews from the `amazon-fine-foods.csv` dataset using both Euclidean distance and cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (3pts)\n",
    "\n",
    "You are an NLP data scientist working at Fandango. You observe the following dataset in your review comments:\n",
    "\n",
    "**Intent to Buy Tickets:**\n",
    "1.\tLove this movie. Can’t wait!\n",
    "2.\tI want to see this movie so bad.\n",
    "3.\tThis movie looks amazing.\n",
    "\n",
    "**No Intent to Buy Tickets:**\n",
    "1.\tLooks bad.\n",
    "2.\tHard pass to see this bad movie.\n",
    "3.\tSo boring!\n",
    "\n",
    "You can consider the following stopwords for removal: `to`, `this`.\n",
    "\n",
    "Is the following review an `Intent to Buy` or `No Intent to Buy`? Show your work for each computation.\n",
    "> This looks so bad.\n",
    "\n",
    "You'll need to compute:\n",
    "* Prior\n",
    "* Likelihood\n",
    "* Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
